{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEu Score 계산을 위한 라이브러리 업데이트\n",
    "번역한 문장이 얼마나 유사한지를 평가해주는 평가 척도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.6.0 in /opt/conda/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from torchtext==0.6.0) (1.11.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchtext==0.6.0) (1.21.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from torchtext==0.6.0) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (from torchtext==0.6.0) (0.1.96)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from torchtext==0.6.0) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchtext==0.6.0) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.6.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.6.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.6.0) (1.26.7)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->torchtext==0.6.0) (3.10.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCY 라이브러리 : Tokenization, tagging 을 위한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm') # 영어 토큰\n",
    "spacy_de = spacy.load('de_core_news_sm') # 독일어 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : I\n",
      " 1 : am\n",
      " 2 : a\n",
      " 3 : graduate\n",
      " 4 : student\n",
      " 5 : .\n"
     ]
    }
   ],
   "source": [
    "# spacy객체를 생성해 간단한 토큰화\n",
    "tokenized= spacy_en.tokenizer('I am a graduate student.')\n",
    "\n",
    "for i, token in enumerate(tokenized):\n",
    "    print(f' {i} : {token.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 독일어를 토큰화 함수 정의\n",
    "def tokenized_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenized_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field 라이브러리를 통해 데이터셋에 대한 구체적인 전처리 내용 명시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# Source 독일어 , Target 영어\n",
    "# 단어의 처음과 끝은 sos, eos, 모두 소문자로 변환, Transform함수의 batch를 위해 batch를 맨앞으로 True\n",
    "SRC = Field(tokenize=tokenized_de, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "TRG = Field(tokenize=tokenized_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:03<00:00, 392kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 97.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 92.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Multi30k 를 불러오기, SRC,TRG 전처리를 사용\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = Multi30k.splits(root='./data',exts=(\".de\",\".en\"), fields=(SRC,TRG))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set length : 29000\n",
      "valid set length: 1014\n",
      "test dataset length: 1000\n",
      "['ein', 'mann', ',', 'der', 'mit', 'einer', 'tasse', 'kaffee', 'an', 'einem', 'urinal', 'steht', '.']\n",
      "['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'train set length : {len(train_dataset)}')\n",
    "print(f'valid set length: {len(valid_dataset)}')\n",
    "print(f'test dataset length: {len(test_dataset)}')\n",
    "\n",
    "print(vars(train_dataset.examples[30])['src'])\n",
    "print(vars(train_dataset.examples[30])['trg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "field 객체의 build_vocab 메서드를 통해 영어와 독어의 단어 사전 생성   \n",
    "-최소 2번이상의 단어만을 선택, input의 demension 크기 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(SRC) : 7853\n",
      "len(TRG) : 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_dataset,min_freq=2)\n",
    "TRG.build_vocab(train_dataset,min_freq=2)\n",
    "\n",
    "print(f'len(SRC) : {len(SRC.vocab)}')\n",
    "print(f'len(TRG) : {len(TRG.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4112\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.stoi[TRG.pad_token]) #unknown token\n",
    "print(TRG.vocab.stoi['<sos>'])\n",
    "print(TRG.vocab.stoi['<eos>'])\n",
    "print(TRG.vocab.stoi['hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한문장에 포함된 단어가 순서대로 나열된 상태로 네트워크에 입력 되어야합니다.   \n",
    "- 하나의 배치에 포함된 문장들이 가지는 단어개수가 유사하도록 만들고   \n",
    "- 이를 Bucketlterator를 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Dataloder 와 유사한 사용\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 배치의 크기 : torch.Size([128, 29])\n",
      "문장의 개수 : 29\n",
      "index 0 : 2\n",
      "index 1 : 5\n",
      "index 2 : 177\n",
      "index 3 : 25\n",
      "index 4 : 0\n",
      "index 5 : 127\n",
      "index 6 : 1475\n",
      "index 7 : 7\n",
      "index 8 : 14\n",
      "index 9 : 1521\n",
      "index 10 : 4\n",
      "index 11 : 3\n",
      "index 12 : 1\n",
      "index 13 : 1\n",
      "index 14 : 1\n",
      "index 15 : 1\n",
      "index 16 : 1\n",
      "index 17 : 1\n",
      "index 18 : 1\n",
      "index 19 : 1\n",
      "index 20 : 1\n",
      "index 21 : 1\n",
      "index 22 : 1\n",
      "index 23 : 1\n",
      "index 24 : 1\n",
      "index 25 : 1\n",
      "index 26 : 1\n",
      "index 27 : 1\n",
      "index 28 : 1\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    src = batch.src\n",
    "    trg = batch.trg\n",
    "\n",
    "    print(f'첫번째 배치의 크기 : {src.shape}')\n",
    "    print(f'문장의 개수 : {src.shape[1]}')\n",
    "\n",
    "    # 현재 배치에 있는 0번 문장에 대한 정보 출력\n",
    "    # 첫번쨰 index0 = 2 : sos, index16 = 3 : eos로 문장 끝\n",
    "    for i in range(src.shape[1]):\n",
    "        print(f'index {i} : {src[0][i].item()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Head Attention 아키텍쳐   \n",
    "- Queriy, key, value를 받는다. 모두 같은 차원   \n",
    "   \n",
    "- Hyperparameter   \n",
    "-- hidden_dim : 하나의 단어에 대한 임베딩 차원   \n",
    "-- n_heads : head의 개수   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_rate, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % n_heads==0\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "\n",
    "        self.fc_Q = nn.Linear(hidden_dim, hidden_dim) # Query\n",
    "        self.fc_K = nn.Linear(hidden_dim, hidden_dim) # Key\n",
    "        self.fc_V = nn.Linear(hidden_dim, hidden_dim) # value 계산 FC layer\n",
    "\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # Q,K,V에 해당하는 값에 루트 한 값을 나누어 주어서 softmax에 되도록\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value , mask =None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_Q(query)\n",
    "        K = self.fc_K(key)\n",
    "        V = self.fc_V(value)\n",
    "        # h개가 각각 하나씩 들어가도록\n",
    "        Q = Q.view(batch_size,-1, self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        K = K.view(batch_size,-1, self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        V = V.view(batch_size,-1, self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        # attention Energy 계산\n",
    "        energy = torch.matmul(Q,K.permute(0,1,3,2)) / self.scale\n",
    "        # energy = [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 결과값이 0인 부분을 아주 작은 값으로 채우기\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask==0, -1e10)\n",
    "        # attention 계산, 각 단어에 대한 확률값\n",
    "\n",
    "        attention = torch.softmax(energy, dim= -1)\n",
    "        # attention = [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        # x = [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        x = x.permute(0,2,1,3).contiguous()\n",
    "        # x = [batch_size, query_len, n_heads, head_dim]\n",
    "        \n",
    "        x = x.view(batch_size,-1,self.hidden_dim)\n",
    "        # x = [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "        # x= [batch_size, query_len, hidden_dim]\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position-wise Feedforward\n",
    "- 입력과 출력의 차원이 동일 합니다   \n",
    "-- hidden_dim : 하나의 단어에 대한 임베딩 차원   \n",
    "-- pf_dim : Feedforward 레이어에서의 내부 임베딩 차원   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforward(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_rate):    \n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x= self.fc2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode layer    \n",
    "-- 입력과 출력의 차원이 같습니다.   \n",
    "-- 인코더 레이어를 여러번 중첩해 사용한다.\n",
    "- 파라미터   \n",
    "-- hidden_dim : 하나의 단어에 대한 임베딩 차원   \n",
    "-- n_head : head의 개수   \n",
    "-- pf_dim : Feedforward 레이어에서의 내부 임베딩 차원   \n",
    "<pad> 토큰에 대하여 mask값을 0으로 설정합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_rate,device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttention(hidden_dim, n_heads, dropout_rate,device)\n",
    "        self.position_feedforward = PositionwiseFeedforward(hidden_dim, pf_dim, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src = [batch_size, src_len, hidden_dim]\n",
    "        # src_mask = [batch_size, src_len]\n",
    "\n",
    "        # self.attention\n",
    "        _src, _ = self.self_attention(src,src,src,src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attention_norm(src+self.dropout(_src))\n",
    "\n",
    "        # src = [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # position\n",
    "        _src = self.position_feedforward(src)\n",
    "\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 Encoder    \n",
    "- 전체 인코더를 정의합니다.   \n",
    "-- input dim : 하나의 단어에 대한 원 핫 인코딩 차원   \n",
    "-- hidden_dim : 하나의 단어에 대한 임베딩 차원   \n",
    "-- n_layer : 내부적으로 사용할 인코더 레이어의 개수   \n",
    "-- n_head : head의 개수   \n",
    "-- pf_dim : Feedforward 레이어에서의 내부 임베딩 차원   \n",
    "-- max_length : 문장 내 최대 단어의 개수   \n",
    "- positional embedding도 학습할 수 있음. BERT와 같은 모델에서 사용되는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_rate, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        # 위치 position vector도 학습\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "        # n_layer 개수 만큼 반복\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim,n_heads,pf_dim,dropout_rate, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, src,src_mask):\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0,src_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
    "        # pos = [batch_size, src_len]\n",
    "\n",
    "        # 소스 문장의 임베딩과 위치 임베딩을 더함\n",
    "        src = self.tok_embedding(src) * self.scale\n",
    "        \n",
    "        src = self.dropout(src)\n",
    "        \n",
    "        # 모든 레이어를 지나가면서 계산\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder layer\n",
    "- 입력과 출력 차원이 같다   \n",
    "- 디코더 레이어를 여러번 중첩해서 사용   \n",
    "- 디코더 레이어는 두개의 Multi-Head attention이 사용됨   \n",
    "-- hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "-- n_heads : head의 개수   \n",
    "-- pd_dim : Feedforward 레이어 에서 내부 임베딩 차원\n",
    "- 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderlayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_rate, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.encode_atten_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.feedfor_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttention(hidden_dim,n_heads,dropout_rate,device)\n",
    "        self.encoder_attention = MultiHeadAttention(hidden_dim,n_heads,dropout_rate,device)\n",
    "        self.position_feedforward = PositionwiseFeedforward(hidden_dim,pf_dim, dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # encoder의 출력값enc_src를 attention 하는 구조\n",
    "\n",
    "        # trg =[ batch_size, trg_len, hidden_dim]\n",
    "        # enc_src = [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask = [batch_size, trg_len]\n",
    "        # src_mask = [batch_size, src_len]\n",
    "\n",
    "        # self attention\n",
    "        _trg, _ = self.self_attention(trg,trg,trg,trg_mask)\n",
    "        _trg = self.dropout(_trg)\n",
    "        trg = self.self_attention_norm(trg + _trg)\n",
    "\n",
    "        # trg = [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        _trg = self.dropout(_trg)\n",
    "        trg = self.encode_atten_norm(trg+_trg)\n",
    "\n",
    "        # position feedforward\n",
    "        _trg = self.position_feedforward(trg)\n",
    "        _trg = self.dropout(_trg)\n",
    "        trg = self.feedfor_layer_norm(trg + _trg)\n",
    "\n",
    "        # trg = [batch_size, trg_len, hidden_dim]\n",
    "        # attention = [batch_size, n_heads, trg_len, src_len]\n",
    "        return trg, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 Decoder\n",
    "- decoder도 incoder와 같이 반복적으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_rate, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([Decoderlayer(hidden_dim,n_heads,pf_dim,dropout_rate,device) for _ in range(n_layers)])\n",
    "        self.fc_out= nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "    def forward(self,trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch_size, trg_len]\n",
    "        # enc_src = [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask = [batch_size, trg_len]\n",
    "        # src_mask = [batch_size, src_len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        posision= torch.arange(0,trg_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
    "        #posision = [batch_size, trg_len]\n",
    "\n",
    "        trg = self.tok_embedding(trg)* self.scale\n",
    "        trg = trg + self.pos_embedding(posision)\n",
    "        trg = self.dropout(trg)\n",
    "        # trg [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            # trg [batch_size, trg_len, hidden_dim]\n",
    "            # attention = [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        return output,attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 Transformer / encoder+decoder   \n",
    "- encoder와 decoder를 거쳐 최종 output 문장을 생성합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device= device\n",
    "    \n",
    "    # Src 문장에서 <pad> 에 대한 문자에 대해 마스크 값을 0으로 설정\n",
    "    def src_mask(self,src):\n",
    "        src_mask = (src !=self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask = [batch_size, 1, 1, src_len]\n",
    "        return src_mask\n",
    "    # trg 문장에서 다음 단어가 무엇인지 알 수 없도록 마스크 사용\n",
    "    def trg_mask(self,trg):\n",
    "        # 마스크 예시, 하나씩 1로 바꾼다.\n",
    "        # 1 0 0 0 0\n",
    "        # 1 1 0 0 0\n",
    "        # 1 1 1 0 0\n",
    "        # 1 1 1 1 0\n",
    "        # 1 1 1 1 1\n",
    "\n",
    "        # <pad>인 것들을 0으로 변경\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask = [batch_size, 1, 1, trg_len]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "        # 예시와 같은 sub mask 생성\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len,trg_len), device=self.device)).bool()\n",
    "        # trg_pad_mask = [trg_len, trg_len]\n",
    "\n",
    "        # 두개의 mask를 and 연산 bitwise \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        # trg_mask = [batch_size, 1, trg_len. trg_len]\n",
    "        return trg_mask\n",
    "    def forward(self, src, trg):\n",
    "        # src [batch_size , src_len]\n",
    "        # trg [batch_size , trg_len]\n",
    "\n",
    "        src_mask = self.src_mask(src)\n",
    "        trg_mask = self.trg_mask(trg)\n",
    "\n",
    "        # src_mask [batch_size, 1, 1, src_len]\n",
    "        # trg_mask [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        # enc_src [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        # output = [batch_size, trg_len, output_dim]\n",
    "        # attention = [batch_size, n_heads, trg_len, src_len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trainable parameters : 9038341\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAVERS = 3\n",
    "DEC_LABERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "# encoder decoder 객체 선언\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAVERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "decoder = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LABERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "# Transformer 선언\n",
    "model = Transformer(encoder, decoder, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "\n",
    "\n",
    "def sum_parameter(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Model trainable parameters : {sum_parameter(model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(7853, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (position_feedforward): PositionwiseFeedforward(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (position_feedforward): PositionwiseFeedforward(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (position_feedforward): PositionwiseFeedforward(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(5893, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): Decoderlayer(\n",
       "        (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (encode_atten_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedfor_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (position_feedforward): PositionwiseFeedforward(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): Decoderlayer(\n",
       "        (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (encode_atten_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedfor_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (position_feedforward): PositionwiseFeedforward(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): Decoderlayer(\n",
       "        (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (encode_atten_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedfor_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (fc_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (position_feedforward): PositionwiseFeedforward(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 초기화\n",
    "def init_weights(model):\n",
    "    # hasattr(object, name) , object 에서 name의 속성 확인\n",
    "    if hasattr(model,'weight') and model.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(model.weight.data)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 패딩 값에 대해서는 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 함수\n",
    "def train(model, train_iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 맨마지막 <eos>는 제외 하여 \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        # output = [batch_size, trg_len-1, output_dim]\n",
    "        # trg = [batch_size, trg_len]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # 일렬로 쭉 펴기\n",
    "        # output = [batch_size * trg_len-1, output_dim]\n",
    "\n",
    "        # trg에서 <sos>는 제외\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        # trg = [batch_size * trg_len-1]\n",
    "\n",
    "        loss = criterion(output,trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping 진행\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate 함수\n",
    "def evaluate(model, val_iterator, criterion):\n",
    "    model.eval()\n",
    "    val_loss=  0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 | Time : 13.9s\n",
      "\tTrain Loss : 4.289 | Train PPL : 72.925\n",
      "\tVal Loss : 3.105 | Val PPL : 22.315\n",
      "Epoch : 2 | Time : 13.9s\n",
      "\tTrain Loss : 2.960 | Train PPL : 19.297\n",
      "\tVal Loss : 2.503 | Val PPL : 12.215\n",
      "Epoch : 3 | Time : 13.8s\n",
      "\tTrain Loss : 2.482 | Train PPL : 11.965\n",
      "\tVal Loss : 2.248 | Val PPL : 9.466\n",
      "Epoch : 4 | Time : 13.8s\n",
      "\tTrain Loss : 2.194 | Train PPL : 8.971\n",
      "\tVal Loss : 2.107 | Val PPL : 8.226\n",
      "Epoch : 5 | Time : 13.9s\n",
      "\tTrain Loss : 1.988 | Train PPL : 7.303\n",
      "\tVal Loss : 2.023 | Val PPL : 7.560\n",
      "Epoch : 6 | Time : 13.9s\n",
      "\tTrain Loss : 1.830 | Train PPL : 6.235\n",
      "\tVal Loss : 1.952 | Val PPL : 7.040\n",
      "Epoch : 7 | Time : 13.9s\n",
      "\tTrain Loss : 1.700 | Train PPL : 5.475\n",
      "\tVal Loss : 1.926 | Val PPL : 6.860\n",
      "Epoch : 8 | Time : 13.9s\n",
      "\tTrain Loss : 1.590 | Train PPL : 4.904\n",
      "\tVal Loss : 1.905 | Val PPL : 6.721\n",
      "Epoch : 9 | Time : 13.9s\n",
      "\tTrain Loss : 1.495 | Train PPL : 4.461\n",
      "\tVal Loss : 1.904 | Val PPL : 6.711\n",
      "Epoch : 10 | Time : 13.9s\n",
      "\tTrain Loss : 1.413 | Train PPL : 4.109\n",
      "\tVal Loss : 1.907 | Val PPL : 6.732\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "history = {}\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_PPL =[]\n",
    "val_PPL = []\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(),'./models/Transformer_de_to_en.pt')\n",
    "    print(f'Epoch : {epoch+1} | Time : {end_time-start_time:.1f}s')\n",
    "    print(f'\\tTrain Loss : {train_loss:.3f} | Train PPL : {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tVal Loss : {valid_loss:.3f} | Val PPL : {math.exp(valid_loss):.3f}')\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(valid_loss)\n",
    "    train_PPL.append(math.exp(train_loss))\n",
    "    val_PPL.append(math.exp(valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "history= {'Train Loss':train_loss_list, 'Val Loss':val_loss_list,'Train PPL':train_PPL,'Val PPL':val_PPL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 1.932 | Test PPL : 6.902\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/Transformer_de_to_en.pt'))\n",
    "\n",
    "test_loss = evaluate(model,test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss : {test_loss:.3f} | Test PPL : {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_translate(sentence, src_field, trg_field, model, device, max_len=50, logging=True):\n",
    "    model.eval()\n",
    "\n",
    "    # str 형이라면 sentence가\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    \n",
    "    tokens = [src_field.init_token]+tokens+[src_field.eos_token]\n",
    "\n",
    "    if logging:\n",
    "        print(f'전체 src tokens : {tokens}')\n",
    "    \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    if logging:\n",
    "        print(f'src 문장 index : {src_indexes}')\n",
    "    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    print(f'src_tensor shaep : {src_tensor.shape}')\n",
    "    # maks 생성\n",
    "    src_mask = model.src_mask(src_tensor)\n",
    "    print(f'src_mask : {src_mask}\\n')\n",
    "    with torch.no_grad():\n",
    "        encoder_src = model.encoder(src_tensor, src_mask)\n",
    "    \n",
    "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        # 출력 문장에 따른 마스크 생성\n",
    "        trg_mask = model.trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, encoder_src, trg_mask, src_mask)\n",
    "            #print(f'output shape : {output.shape}\\n')\n",
    "            # 출력 문장에서 가장 마지막 단어만 사용\n",
    "            pred_token = output.argmax(2)[:,-1].item()\n",
    "            print(f'pred_token : {pred_token}')\n",
    "            trg_indexes.append(pred_token)\n",
    "\n",
    "            # <eos>를 만나면 종료\n",
    "            if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "                break\n",
    "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    # 첫번째 <sos>는 제외하고 출력 문장 반환\n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src 문장 :  eine mutter und ihr kleiner sohn genießen einen schönen tag im freien .\n",
      "trg 문장 :  a mother and her young song enjoying a beautiful day outside .\n",
      "전체 src tokens : ['<sos>', 'eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.', '<eos>']\n",
      "src 문장 index : [2, 8, 364, 10, 134, 70, 624, 565, 19, 780, 200, 20, 88, 4, 3]\n",
      "src_tensor shaep : torch.Size([1, 15])\n",
      "src_mask : tensor([[[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True]]]], device='cuda:0')\n",
      "\n",
      "pred_token : 4\n",
      "pred_token : 496\n",
      "pred_token : 11\n",
      "pred_token : 44\n",
      "pred_token : 707\n",
      "pred_token : 737\n",
      "pred_token : 4\n",
      "pred_token : 999\n",
      "pred_token : 184\n",
      "pred_token : 341\n",
      "pred_token : 5\n",
      "pred_token : 3\n",
      "모델 출력 결과 :  a mother and her son enjoy a nice day outdoors . <eos>\n"
     ]
    }
   ],
   "source": [
    "example_idx = 10\n",
    "\n",
    "ex_src = vars(test_dataset.examples[example_idx])['src']\n",
    "ex_trg = vars(test_dataset.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src 문장 : ',*ex_src)\n",
    "print(f'trg 문장 : ',*ex_trg)\n",
    "\n",
    "trans, attention = example_translate(ex_src,SRC,TRG, model, device, logging=True)\n",
    "print('모델 출력 결과 : ', \" \".join(trans))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
